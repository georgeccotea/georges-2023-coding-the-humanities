{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ff2UYOrk_bd"
   },
   "source": [
    "# Individual Exam CtH | Analysing \"Brand Twitter\"\n",
    "\n",
    "### Author: *George*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlBTVZLP_9cX"
   },
   "source": [
    "Be sure to check the assignment description on Canvas once more.\n",
    "\n",
    "Handing in the notebook should be done in a `zip` file, together with any files you create. We request you to name the exam submission as \"exam_name_studentname.zip\" with your corresponding name. The data is presented in the `data` folder. Also, include the files containing the data when handing in your exam, as this helps us to check your code when re-running it. To check if we can run your notebook from top to bottom without receiving errors, try to clear all the output of the cells and rerun everything. This can be done automatically by clicking on `Runtime --> Restart and run all`.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0z7yBxIPWJc"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "For this assignment we will investigate the phenomenon of \"*Brand Twitter*\". While corporate communication is traditionally rather bland, in recent years a trend has emerged of brands communicating on social media in a far less formal way. Social media teams of certain brands have started use memes, edgy humour and an informal and personal style of writing, sometimes also engaging with other brand accounts in a way that is similar to how many Twitter users tweet and interact with each other. This phenomenon of corporate personhood has been called *Brand Twitter*.\n",
    "\n",
    "In 2019, Vulture published a history of the phenomenon, just in case you find it interesting: https://www.vulture.com/2019/06/brand-twitter-jokes-history.html\n",
    "\n",
    "For this assignment we will consider a group of brand accounts that are often considered part of *Brand Twitter*:\n",
    "\n",
    "    * @Wendys - Wendy's\n",
    "    * @PrimeVideo - Amazon Prime Video\n",
    "    * @MerriamWebster - Merriam Webster\n",
    "    * @BurgerKing - Burger King\n",
    "    * @Netflix - Netflix US\n",
    "    * @McDonalds - McDonald's\n",
    "    * @DennysDiner - Denny's Diner\n",
    "\n",
    "We thought it would be interesting to analyse the tweets of these brand accounts to learn more about this novel style of corporate communication and the ways corporations might be perceived as relatable people on social media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI8_S13FGYWN"
   },
   "source": [
    "# Data\n",
    "\n",
    "The data can be found within the file `brand_tweets.csv`. This file contains twitter data on tweets (every row is one tweet) by these brands.\n",
    "\n",
    "FYI: This data was acquired from Twitter using their API if you're interested, following the method from the optional Notebook 7.\n",
    "\n",
    "*Please note: this dataset might contain content which could be considered as offensive. It is real unfiltered data directly from Twitter.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vwI8QmUKJYk"
   },
   "source": [
    "# Tasks\n",
    "\n",
    "We would like to look through some recent tweets of *Brand Twitter*, and be able to understand certain characteristics of their tweets. As these Twitter accounts represent major brands, one particularly interesting aspect of this dataset is the difference between regular tweets and tweets that are replies to other tweets, which could be replies to customers or other brands.\n",
    "\n",
    "Make your code and results as pretty as possible, and feel free to use tabs and enumeration when printing text and formatting for the visualisations. \n",
    "\n",
    "This assignment is about getting familiar with Pandas' methods â€” we suggest going through the lecture and seminar notebooks on Pandas again. You can of course use any course material in this assignment!\n",
    "\n",
    "You are not limited to the structure of the cells below with ` # Your code here` only. Organise your code the way you think is most readible and appropriate.\n",
    "\n",
    "If you do not fully manage to solve a question in the requested way, feel free to solve it in a different way to be able to proceed with later questions - you'll probably still get some points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS58nZzcKA44"
   },
   "source": [
    "### Question 1: Pre-processing\n",
    "* Add a column with a normalised version of the 'text' column. Use an appropriate tokenizer for this type of data in your normalization function. As the 'text' column contains strings, things will be easier if your normalized text column will also contain strings.\n",
    "\n",
    "Continue to work with this normalized column in the next tasks. You're of course free to add more columns if you think you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/george/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/george/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/george/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download the necessary packages and bypassing pycharm bug\n",
    "# https://stackoverflow.com/questions/38916452/nltk-download-ssl-certificate-verify-failed\n",
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-24T13:44:43.597634Z",
     "end_time": "2023-03-24T13:44:43.992305Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DVwFEgSAPN5P",
    "ExecuteTime": {
     "start_time": "2023-03-24T13:45:18.377362Z",
     "end_time": "2023-03-24T13:45:21.609528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0    @pure_fox3 This â€™ service expect . Please DM u...\n1    @violetisaghost We're disappointed hear . Plea...\n2                @Ciabatta_Boi https://t.co/XtTwZVeNcM\n3    @Plentlyofcolin We're sorry read . Can DM u ad...\n4    @Quasimfk We're disappointed hear . Please sen...\n5    @kjdoherty1 We're sorry hear . Please send u D...\n6    @OfSqueal We're sorry hear . Please send u DM ...\n7    @sunm0re0ften We're sorry . Please send u DM o...\n8    @keltothelean That's okay ! DM u restaurant lo...\n9    @MaryNowakATL We're disappointed hear . Please...\nName: normalized_text, dtype: object"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "#Importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#Opening the csv and putting it into a data frame for later processing\n",
    "df = pd.read_csv('data/brand_tweets.csv')\n",
    "df.head()\n",
    "#Creating a new column for the normalized text\n",
    "df['normalized_text'] = df['text']\n",
    "#Creating a function to normalize the text\n",
    "def normalize_text(text):\n",
    "    #Tokenizing the text\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    #Removing stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    #Lemmatizing the text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    #Stemming the text\n",
    "    #stemmer = PorterStemmer()\n",
    "    #tokens = [stemmer.stem(token) for token in tokens]\n",
    "    #Joining the text\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "#Applying the function to the normalized text column\n",
    "df['normalized_text'] = df['normalized_text'].apply(normalize_text)\n",
    "df.head()\n",
    "#Printing the first 10 rows of the normalized text column for testing\n",
    "df['normalized_text'].head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c05kDUHdOCkV"
   },
   "source": [
    "### Question 2: Description / statistics\n",
    "\n",
    "Provide information on: \n",
    "\n",
    "1. Number of tweets (per brand and in total)\n",
    "2. How many of those tweets are replies (per brand and in total)\n",
    "3. Most liked tweet (per brand and in total)\n",
    "4. Most frequent hashtags (per brand and in total)\n",
    "\n",
    "You can present the answers in this notebook. If you prefer, you may also write your results to a separate text file (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Number of tweets (per brand and in total)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_xdsHrGqOBP5",
    "ExecuteTime": {
     "start_time": "2023-03-24T13:46:46.799627Z",
     "end_time": "2023-03-24T13:46:46.828686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "BurgerKing        3248\nMcDonalds         3248\nWendys            3246\nDennysDiner       3210\nMerriamWebster    3169\nPrimeVideo        1966\nNetflix           1828\nName: username, dtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing the number of tweets per brand\n",
    "df['username'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### How many of those tweets are replies (per brand and in total)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per brand, the number of replies is the following:\n",
      "Wendys:  3174\n",
      "Prime Video:  543\n",
      "Merriam Webster:  1244\n",
      "Burger King:  3192\n",
      "Netflix:  362\n",
      "McDonalds:  3245\n",
      "Dennys Diner:  2349\n",
      "The total number of replies is:  14109\n"
     ]
    }
   ],
   "source": [
    "#Separating the dataframes per username\n",
    "df_wendys = df[df['username'] == 'Wendys']\n",
    "df_primevideo = df[df['username'] == 'PrimeVideo']\n",
    "df_merriamwebster = df[df['username'] == 'MerriamWebster']\n",
    "df_burgerking = df[df['username'] == 'BurgerKing']\n",
    "df_netflix = df[df['username'] == 'Netflix']\n",
    "df_mcdonalds = df[df['username'] == 'McDonalds']\n",
    "df_dennysdiner = df[df['username'] == 'DennysDiner']\n",
    "#Counting the number of times in_reply_to_user is not null per username\n",
    "wendysReply = df_wendys['in_reply_to_user'].notnull().sum()\n",
    "primeReply = df_primevideo['in_reply_to_user'].notnull().sum()\n",
    "merriamReply = df_merriamwebster['in_reply_to_user'].notnull().sum()\n",
    "bkReply = df_burgerking['in_reply_to_user'].notnull().sum()\n",
    "netflixReply = df_netflix['in_reply_to_user'].notnull().sum()\n",
    "mecReply = df_mcdonalds['in_reply_to_user'].notnull().sum()\n",
    "dennysReply = df_dennysdiner['in_reply_to_user'].notnull().sum()\n",
    "\n",
    "print(\"Per brand, the number of replies is the following:\")\n",
    "print(\"Wendys: \", wendysReply)\n",
    "print(\"Prime Video: \", primeReply)\n",
    "print(\"Merriam Webster: \", merriamReply)\n",
    "print(\"Burger King: \", bkReply)\n",
    "print(\"Netflix: \", netflixReply)\n",
    "print(\"McDonalds: \", mecReply)\n",
    "print(\"Dennys Diner: \", dennysReply)\n",
    "\n",
    "#Totalling the number of replies\n",
    "totalReplies = wendysReply + primeReply + merriamReply + bkReply + netflixReply + mecReply + dennysReply\n",
    "print(\"The total number of replies is: \", totalReplies)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-24T13:56:36.485668Z",
     "end_time": "2023-03-24T13:56:36.499460Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Most liked tweet (per brand and in total)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most liked tweet per brand is the following:\n",
      "Wendys:  .@elonmusk let me tweet from space pls\n",
      "Prime Video:  Like if you can hear this image. https://t.co/gusrJ77Abr\n",
      "Merriam Webster:  ONE OF THE DEFINITIONS OF 'LITERALLY' IS \"IN EFFECT, VIRTUALLYâ€”USED IN AN EXAGGERATED WAY TO EMPHASIZE A STATEMENT OR DESCRIPTION.\" SOME PEOPLE GET MAD IF YOU USE IT THIS WAY BECAUSE IT ISN'T THE WORD'S PRIMARY MEANING, BUT THIS SENSE OF 'LITERALLY' HAS BEEN USED FOR 250 YEARS SO https://t.co/teDuHmxtrk\n",
      "Burger King:  bk boss: did you write those tweets i asked for?\n",
      "\n",
      "my brain: whopper whopper whopper whopper\n",
      "Netflix:  Wednesday has been officially renewed for Season 2! https://t.co/ekqlxP9ueW\n",
      "McDonalds:  i read all the comments ðŸ«¶ https://t.co/6mLhl44v1L\n",
      "Dennys Diner:  find cute baby from viral video and bring him to Dennyâ€™s â€¦ âœ… https://t.co/f6BvdfnzaB\n",
      "The most liked tweet in total is:  Wednesday has been officially renewed for Season 2! https://t.co/ekqlxP9ueW\n"
     ]
    }
   ],
   "source": [
    "#Finding the most liked tweet per brand\n",
    "mostLikedWendys = df_wendys['like_count'].max()\n",
    "mostLikedPrime = df_primevideo['like_count'].max()\n",
    "mostLikedMerriam = df_merriamwebster['like_count'].max()\n",
    "mostLikedBK = df_burgerking['like_count'].max()\n",
    "mostLikedNetflix = df_netflix['like_count'].max()\n",
    "mostLikedMec = df_mcdonalds['like_count'].max()\n",
    "mostLikedDennys = df_dennysdiner['like_count'].max()\n",
    "#Associating the most liked tweet with the brand\n",
    "mostLikedWendys = df_wendys[df_wendys['like_count'] == mostLikedWendys]['text'].values[0]\n",
    "mostLikedPrime = df_primevideo[df_primevideo['like_count'] == mostLikedPrime]['text'].values[0]\n",
    "mostLikedMerriam = df_merriamwebster[df_merriamwebster['like_count'] == mostLikedMerriam]['text'].values[0]\n",
    "mostLikedBK = df_burgerking[df_burgerking['like_count'] == mostLikedBK]['text'].values[0]\n",
    "mostLikedNetflix = df_netflix[df_netflix['like_count'] == mostLikedNetflix]['text'].values[0]\n",
    "mostLikedMec = df_mcdonalds[df_mcdonalds['like_count'] == mostLikedMec]['text'].values[0]\n",
    "mostLikedDennys = df_dennysdiner[df_dennysdiner['like_count'] == mostLikedDennys]['text'].values[0]\n",
    "#Printing the most liked tweet per brand\n",
    "print(\"The most liked tweet per brand is the following:\")\n",
    "print(\"Wendys: \", mostLikedWendys)\n",
    "print(\"Prime Video: \", mostLikedPrime)\n",
    "print(\"Merriam Webster: \", mostLikedMerriam)\n",
    "print(\"Burger King: \", mostLikedBK)\n",
    "print(\"Netflix: \", mostLikedNetflix)\n",
    "print(\"McDonalds: \", mostLikedMec)\n",
    "print(\"Dennys Diner: \", mostLikedDennys)\n",
    "#Finding the most liked tweet in total\n",
    "mostLikedTotal = df['like_count'].max()\n",
    "#Associating the most liked tweet with the text\n",
    "mostLikedTotal = df[df['like_count'] == mostLikedTotal]['text'].values[0]\n",
    "#Printing the most liked tweet in total\n",
    "print(\"The most liked tweet in total is: \", mostLikedTotal)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-24T14:22:18.310428Z",
     "end_time": "2023-03-24T14:22:18.315076Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Most frequent hashtags (per brand and in total)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent hashtags per brand are the following:\n",
      "Wendys:  hashtags          \n",
      "WendysHotandCrispy    43\n",
      "ChooseHotAndCrispy    22\n",
      "WendysHotAndCrispy     6\n",
      "ChooseHotandCrispy     4\n",
      "dtype: int64\n",
      "Prime Video:  hashtags       \n",
      "MyPoliceman        68\n",
      "TheRingsOfPower    58\n",
      "TNFonPrime         53\n",
      "ThePeripheral      40\n",
      "HarlemOnPrime      33\n",
      "dtype: int64\n",
      "Merriam Webster:  hashtags    \n",
      "WordOfTheDay    714\n",
      "SpellingBee      28\n",
      "etymology        10\n",
      "PopRhetoric       9\n",
      "WordWellUsed      7\n",
      "dtype: int64\n",
      "Burger King:  hashtags     \n",
      "BurgerKingYes    19\n",
      "yourule           8\n",
      "BKContest         5\n",
      "YouRule           5\n",
      "BKWarmCore        3\n",
      "dtype: int64\n",
      "Netflix:  hashtags           \n",
      "TUDUM                  74\n",
      "SAGAwards              37\n",
      "NetflixSaveTheDates    19\n",
      "ChrisRockLive           7\n",
      "EVsOnScreen             7\n",
      "dtype: int64\n",
      "McDonalds:  Series([], dtype: int64)\n",
      "Dennys Diner:  hashtags       \n",
      "DennysFC           21\n",
      "DennysBreakFast    20\n",
      "HappyDaddysDay     17\n",
      "NFTforNKH           6\n",
      "TheMatrix           6\n",
      "dtype: int64\n",
      "The most frequent hashtags in total are the following:\n",
      "WordOfTheDay       714\n",
      "TUDUM               74\n",
      "MyPoliceman         68\n",
      "TheRingsOfPower     58\n",
      "TNFonPrime          53\n",
      "dtype: int64\n",
      "The most frequent hashtags in total are the following:\n",
      "WordOfTheDay       714\n",
      "TUDUM               74\n",
      "MyPoliceman         68\n",
      "TheRingsOfPower     58\n",
      "TNFonPrime          53\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Finding the most frequent hashtags per brand from the normalized text column\n",
    "mostFreqWendys = df_wendys['normalized_text'].str.findall(r\"#(\\w+)\").sum()\n",
    "mostFreqPrime = df_primevideo['normalized_text'].str.findall(r\"#(\\w+)\").sum()\n",
    "mostFreqMerriam = df_merriamwebster['normalized_text'].str.findall(r\"#(\\w+)\").sum()\n",
    "mostFreqBK = df_burgerking['normalized_text'].str.findall(r\"#(\\w+)\").sum()\n",
    "mostFreqNetflix = df_netflix['normalized_text'].str.findall(r\"#(\\w+)\").sum()\n",
    "mostFreqMec = df_mcdonalds['normalized_text'].str.findall(r\"#(\\w+)\").sum()\n",
    "mostFreqDennys = df_dennysdiner['normalized_text'].str.findall(r\"#(\\w+)\").sum()\n",
    "#Finding the hashtags that are present more than once per brand\n",
    "mostFreqWendys = [i for i in mostFreqWendys if mostFreqWendys.count(i) > 1]\n",
    "mostFreqPrime = [i for i in mostFreqPrime if mostFreqPrime.count(i) > 1]\n",
    "mostFreqMerriam = [i for i in mostFreqMerriam if mostFreqMerriam.count(i) > 1]\n",
    "mostFreqBK = [i for i in mostFreqBK if mostFreqBK.count(i) > 1]\n",
    "mostFreqNetflix = [i for i in mostFreqNetflix if mostFreqNetflix.count(i) > 1]\n",
    "mostFreqMec = [i for i in mostFreqMec if mostFreqMec.count(i) > 1]\n",
    "mostFreqDennys = [i for i in mostFreqDennys if mostFreqDennys.count(i) > 1]\n",
    "#Putting the most frequent hashtags in a pandas dataframe and finding the top 5 most frequent hashtags per brand\n",
    "mostFreqWendys = pd.DataFrame(mostFreqWendys, columns=['hashtags']).value_counts()[:5]\n",
    "mostFreqPrime = pd.DataFrame(mostFreqPrime, columns=['hashtags']).value_counts()[:5]\n",
    "mostFreqMerriam = pd.DataFrame(mostFreqMerriam, columns=['hashtags']).value_counts()[:5]\n",
    "mostFreqBK = pd.DataFrame(mostFreqBK, columns=['hashtags']).value_counts()[:5]\n",
    "mostFreqNetflix = pd.DataFrame(mostFreqNetflix, columns=['hashtags']).value_counts()[:5]\n",
    "mostFreqMec = pd.DataFrame(mostFreqMec, columns=['hashtags']).value_counts()[:5]\n",
    "mostFreqDennys = pd.DataFrame(mostFreqDennys, columns=['hashtags']).value_counts()[:5]\n",
    "#Printing the most frequent hashtags per brand\n",
    "print(\"The most frequent hashtags per brand are the following:\")\n",
    "print(\"Wendys: \", mostFreqWendys)\n",
    "print(\"Prime Video: \", mostFreqPrime)\n",
    "print(\"Merriam Webster: \", mostFreqMerriam)\n",
    "print(\"Burger King: \", mostFreqBK)\n",
    "print(\"Netflix: \", mostFreqNetflix)\n",
    "print(\"McDonalds: \", mostFreqMec)\n",
    "print(\"Dennys Diner: \", mostFreqDennys)\n",
    "#Finding the most frequent hashtags in total from the normalized text column\n",
    "mostFreqTotal = df['normalized_text'].str.findall(r\"#(\\w+)\").sum()\n",
    "#Putting the most frequent hashtags in a series and finding the top 5 most frequent hashtags\n",
    "mostFreqTotal = pd.Series(mostFreqTotal).value_counts()[:5]\n",
    "#Printing the dataframe with the top 5 hastags per brand\n",
    "print(\"The most frequent hashtags in total are the following:\")\n",
    "print(mostFreqTotal)\n",
    "#Printing the dataframe with the most frequent hashtags in total\n",
    "print(\"The most frequent hashtags in total are the following:\")\n",
    "print(mostFreqTotal)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-24T14:28:28.646884Z",
     "end_time": "2023-03-24T14:28:28.716717Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Question 3: Analysis - Corporate personhood\n",
    "\n",
    "To observe to what extent the brands encourage corporate personhood, it would be interesting to see what pronouns the brands use to refer to themselves: \"we\" or \"I\". Let us define a \"First Person Pronoun Ratio\" - the total number of times that the word \"I\" is used by a brand / the total number of times the words \"I\" or \"we\" are used by a brand. This should give us a value between 0 and 1, and a higher value indicates that the brand used the word \"I\" relatively more often compared to the word \"we\". If we multiply this value by 100, it becomes a percentage.\n",
    "\n",
    "1. For all of the 7 brands, compute their First Person Pronoun Ratio (or percentage) as defined here.\n",
    "2. Choose the brand with the highest First Person Pronoun Ratio. For this brand, compute the First Person Pronoun Ratio separately for tweets that are replies and tweets that are not replies.\n",
    "\n",
    "Briefly interpret the result (as a text block)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Person Pronoun Ratio for each brand is the following:\n",
      "Wendys:  0.7764598540145985\n",
      "Prime Video:  0.7335722819593787\n",
      "Merriam Webster:  0.6916208791208791\n",
      "Burger King:  0.7868568958063121\n",
      "Netflix:  0.7913938260056127\n",
      "McDonalds:  0.30393996247654786\n",
      "Dennys Diner:  0.6832174776564052\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_wendysReplies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 31\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDennys Diner: \u001B[39m\u001B[38;5;124m\"\u001B[39m, firstPersonPronounRatioDennys)\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m#Computing the First Person Pronoun Ratio for each brand for tweets that are replies\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m firstPersonPronounRatioWendysReplies \u001B[38;5;241m=\u001B[39m firstPersonPronounRatio(\u001B[43mdf_wendysReplies\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnormalized_text\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mcat(sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     32\u001B[0m firstPersonPronounRatioPrimeReplies \u001B[38;5;241m=\u001B[39m firstPersonPronounRatio(df_primevideo_replies[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnormalized_text\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mcat(sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     33\u001B[0m firstPersonPronounRatioMerriamReplies \u001B[38;5;241m=\u001B[39m firstPersonPronounRatio(df_merriamwebster_replies[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnormalized_text\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mcat(sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_wendysReplies' is not defined"
     ]
    }
   ],
   "source": [
    "#Creating a function to compute the First Person Pronoun Ratio\n",
    "def firstPersonPronounRatio(text):\n",
    "    #Counting the number of times the word \"I\" is used\n",
    "    iCount = text.count(\"I\")\n",
    "    #Counting the number of times the word \"we\" is used\n",
    "    weCount = text.count(\"we\")\n",
    "    #Counting the number of times the word \"I\" or \"we\" is used\n",
    "    iWeCount = iCount + weCount\n",
    "    #Computing the First Person Pronoun Ratio\n",
    "    firstPersonPronounRatio = iCount / iWeCount\n",
    "    #Returning the First Person Pronoun Ratio\n",
    "    return firstPersonPronounRatio\n",
    "#Computing the First Person Pronoun Ratio for each brand\n",
    "firstPersonPronounRatioWendys = firstPersonPronounRatio(df_wendys['normalized_text'].str.cat(sep=' '))\n",
    "firstPersonPronounRatioPrime = firstPersonPronounRatio(df_primevideo['normalized_text'].str.cat(sep=' '))\n",
    "firstPersonPronounRatioMerriam = firstPersonPronounRatio(df_merriamwebster['normalized_text'].str.cat(sep=' '))\n",
    "firstPersonPronounRatioBK = firstPersonPronounRatio(df_burgerking['normalized_text'].str.cat(sep=' '))\n",
    "firstPersonPronounRatioNetflix = firstPersonPronounRatio(df_netflix['normalized_text'].str.cat(sep=' '))\n",
    "firstPersonPronounRatioMec = firstPersonPronounRatio(df_mcdonalds['normalized_text'].str.cat(sep=' '))\n",
    "firstPersonPronounRatioDennys = firstPersonPronounRatio(df_dennysdiner['normalized_text'].str.cat(sep=' '))\n",
    "#Printing the First Person Pronoun Ratio for each brand\n",
    "print(\"The First Person Pronoun Ratio for each brand is the following:\")\n",
    "print(\"Wendys: \", firstPersonPronounRatioWendys)\n",
    "print(\"Prime Video: \", firstPersonPronounRatioPrime)\n",
    "print(\"Merriam Webster: \", firstPersonPronounRatioMerriam)\n",
    "print(\"Burger King: \", firstPersonPronounRatioBK)\n",
    "print(\"Netflix: \", firstPersonPronounRatioNetflix)\n",
    "print(\"McDonalds: \", firstPersonPronounRatioMec)\n",
    "print(\"Dennys Diner: \", firstPersonPronounRatioDennys)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## To the idiot (me) that is coding this right now, please note that you have not finished this thing as your poor ass code never stored the replies in a dataframed like a civilised person would do."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yy4BDoPAOcp9"
   },
   "source": [
    "### Question 4: Analysis - Brand interaction\n",
    "\n",
    "Among the tweets that are replies, do the *Brand Twitter* brands reply to each other? (in a tweet this is done by writing @username at the beginning of the tweet)\n",
    "\n",
    "1. Print three tweets from the dataframe in which a brand mentions one of the other brands.\n",
    "2. For all of the 7 brands, find out how often they mention each of the other brands in replies. Present the result as a DataFrame.\n",
    " \n",
    "Briefly interpret the result (as a text block)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fraxfplOo4i"
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRYM_Nr2Osxw"
   },
   "source": [
    "### Question 5: Visualization\n",
    "\n",
    "1. Plot the number of tweets in the whole dataset per week. \n",
    "    * Interpret the graph. Can you explain the overall pattern and/or some of the fluctuations that are visible? Feel free to also make reference to the numbers you computed for Question 2 in your explanation.\n",
    "    * (If needed, restrict the dataframe to an active twitter timeframe)\n",
    "\n",
    "2. Choose one of the seven brands and plot its (Twitter) popularity over time (choose the time unit and range of your choice) by:\n",
    "    * Number of retweets\n",
    "    * Number of likes\n",
    "  \n",
    "  You can either try to plot these two metrics (retweets/likes) in the same figure, or create multiple figures.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdQOJkcVOb7b"
   },
   "outputs": [],
   "source": [
    "#Your code here\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exam_CtH_2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
